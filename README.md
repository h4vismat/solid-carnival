# solid-carnival
A small project containing an ETL pipeline within AWS using Terraform.

### AWS Services Used
* Glue: Distributed ETL with Spark and table catalog.
* S3: Data archiving.
* Athena: Serverless data analysis with SQL integrated with various file types.
* Cloudwatch: Job monitoring and alerts.
* SNS: Email notification for Cloudwatch alerts.
* IAM: Permissions.

### Other Services Used
* Terraform: Infrastructure provisioning through code.

### How to set up this project?
You will need:
1. An AWS account.
2. Access Key and Secret Key generated by IAM for Terraform.
3. An email address to receive notifications.

From the project's root directory:
```
cd src/tf/
terraform apply
```

You will be prompted for variables to set up the environment.
You may run the workflow through the AWS console. The default name of the workflow is sales-workflow-glue.
Queries are to be made with Athena. Partitioning should be taken into account when querying.

### Where to view the architecture diagram?
The architecture diagram is in the .drawio file. Use the app.diagrams.net to view it.

## Technical Explanations
### ETL
The project uses AWS Glue to perform ETL on an initial file, sales_data.csv (located in files/sales_data.csv).
Glue is used due to its capacity to transform data in a quasi-serverless manner. A developer or a data engineer does not need to worry about the underlying infrastructure, but can scale DPUs and change Spark configurations as needed.

In the first step, the .csv data is converted to .parquet, and fields are transformed into their appropriate types (int, float, etc.).
The landing -> raw transformation stage marks technical-level transformations, ensuring that future transformations only focus on business logic. The dataframe is then stored into the Glue data catalog through a crawler.

Next, the table from the Glue data catalog is consumed by three jobs: refined_customer_revenue.py, refined_monthly_revenue.py and refined_product_sales.py. They all run in parallel, to ensure a more time-efficient transformation, and independent transformation from one another.
These raw -> refined transformations apply business logic, such as retrieving total revenue per month, quantity of sales per product, etc.
The resulting datasets are then crawled to be stored by the Glue data catalog.

Finally, the jobs are then sent to their trusted counterparts, each one of them storing the resulting datasets in Delta format. No business logic is done on this part, this layer is meant to leverage Delta Lake capabilities such as the scalable metadata, schema evolution and audit history.
The trusted layer is also crawled.

I chose to not use Airflow due to the costs associated with MWAA and/or hosting EC2 instances, alongside with the costs related to VPCs.
In a professional setting, Airflow would definitely be the first choice due to the possibility of integrating with various AWS services such as Lambda, SQS, Sagemaker, and others, not only Glue.
Because of this, I have used Glue Workflows to automate the pipeline. The workflow's default name is sales-workflow-glue. A screenshot may be seen at glue_workflow.png.

### Analytics
All the data stored into the Glue Data Catalog by the crawlers can then be queried for analytical purposes.
Athena was used due to being serverless, its seamless integration with the Glue Data Catalog, and also its support for various data formats, be it structure or unstructured data.

### Storage and partitioning
Data is stored in S3. S3 is infinitely scalable, highly durable and highly available.
Data was partitioned in S3 by year/month/day. This was done for two reasons:
1. Considering a daily ETL pipeline, partitioning by day allows one to leverage S3's IOPS that are dependent on key prefixes. By partitioning the data, one can retrieve data from a given day faster.
2. Athena bills its usage by number of bytes scanned per query. Costs can be reduced by specifying which partition I want to look for, reducing the number of bytes scanned in the process.

### Monitoring
Cloudwatch was used as a monitoring tool. Alarms were setup for each of the jobs, leveraging SNS to notify by e-mail if any of the jobs fail.
Cloudwatch and SNS have close integration with AWS services, Glue included, so it was a pretty straightforward decision.

### Queries
Queries can ben found on src/queries/. Athena should be used to run them. Partitioning should be considered while writing queries in a professional setting.
